{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first attempt at implementing the baum welch algorithm for learning a HMM\n",
    "\n",
    "- In the 'make_data' notebook we made up some data from an unfair casino\n",
    "- Now let's try to write the algorithm to estimate the transition matrix and the emission matrix from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5],\n",
       "       [ 0.5,  0.5],\n",
       "       [ 0.5,  0.5]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((3,2))\n",
    "cs = x.sum(axis=1)\n",
    "cst = np.tile(cs, (x.shape[1], 1)).T\n",
    "x /= cst\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def fowards_pass_algorithm(transition_matrix, emission_matrix, observations, init_state_distribution):\n",
    "    \"\"\"\n",
    "    transition_matrix: n_state x n_state matrix\n",
    "    emission_matrix: n_state x n_discrete_observations (unique types of observations, eg sides of a dice, H/T of a coin)\n",
    "    observations: 1d vector of length n_samples\n",
    "    \"\"\"\n",
    "    n_samples = len(observations)\n",
    "    # keep a list of the scaling factors, this way we don't run into underflow errors\n",
    "    scaling_factors = np.zeros(n_samples)\n",
    "    \n",
    "    n_states = transition_matrix.shape[0]\n",
    "    \n",
    "    forward_pass_probabilities = np.zeros((n_samples, n_states))\n",
    "    \n",
    "    # compute the initial forward probabilities\n",
    "    forward_pass_probabilities[0, :] = init_state_distribution * emission_matrix[:, observations[0]]\n",
    "    scaling_factors[0] = forward_pass_probabilities[0, :].sum()\n",
    "    \n",
    "    forward_pass_probabilities[0, :] /= scaling_factors[0]\n",
    "    \n",
    "    # now we compute the probabilities of stuff going forward, using the probabilites from the previous estimate\n",
    "    for current_t in range(1, n_samples):\n",
    "        # first we want to compute the probability of transitioning over all the previous states and now\n",
    "        # t_steps-1 x n_state * n_state x n_state\n",
    "        trans_prob = forward_pass_probabilities[0:current_t-1, :].dot(transition_matrix) \n",
    "        \n",
    "        # compute the sum over the rows of the previous matrix multiplication and then multiply by the \n",
    "        # probabilities associated with the emission matrix\n",
    "        current_state_estimate = trans_prob * emission_matrix[:, observations[current_t]]\n",
    "        forward_pass_probabilities[current_t, :] = current_state_estimate\n",
    "        \n",
    "        # compute the scaling factor and normalize our running forward probabilities\n",
    "        scaling_factors[current_t] = forward_pass_probabilities[current_t, :].sum()\n",
    "        forward_pass_probabilities[current_t, :] /= scaling_factors[current_t]\n",
    "        \n",
    "    # we be done\n",
    "    return forward_pass_probabilities, scaling_factors\n",
    " \n",
    "    \n",
    "def backwards_pass_algorithm(transition_matrix, emission_matrix, observations, scaling_factors):\n",
    "    n_samples = len(observations)\n",
    "    \n",
    "    n_states = transition_matrix.shape[0]\n",
    "    \n",
    "    backward_pass_probabilities = np.zeros((n_samples, n_states))\n",
    "    \n",
    "    # initialize the last row of the backwards pass probabilities \n",
    "    backward_pass_probabilities[-1, :] = 1.0\n",
    "    \n",
    "    for current_t in range(n_samples-1, -1, -1):\n",
    "        trans_prob = transition_matrix.dot(emission_matrix[:, observations[current_t]])\n",
    "        current_state_estimate = backward_pass_probabilities[current_t+1, :] * trans_prob\n",
    "        backward_pass_probabilities[current_t, :] = current_state_estimate\n",
    "        backward_pass_probabilities[current_t] /= scaling_factors[current_t]\n",
    "        \n",
    "    return backward_pass_probabilities\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def baum_welch(labeled_states, \n",
    "               observations, \n",
    "               init_state_distribution=None,\n",
    "               init_transition_matrix=None, \n",
    "               init_emission_matrix=None):\n",
    "    \"\"\"\n",
    "    Returns a 'learned' transition and emission matrix from the supplied data\n",
    "    \"\"\"\n",
    "    \n",
    "    if init_transition_matrix is None:\n",
    "        n_states = len(set(labeled_states))\n",
    "        init_transition_matrix = np.ones(shape=(n_states, n_states))\n",
    "        col_sums = init_transition_matrix.sum(axis=1)\n",
    "        col_sums = np.tile(col_sums, (n_states, 1)).T\n",
    "        init_transition_matrix /= col_sums\n",
    "        \n",
    "    if init_emission_matrix is None:\n",
    "        n_states = len(set(labeled_states))\n",
    "        n_obs = len(set(observations))\n",
    "        init_emission_matrix = np.ones((n_states, n_obs))\n",
    "        col_sums = init_emission_matrix.sum(axis=1)\n",
    "        col_sums = np.tile(col_sums, (init_transition_matrix.shape[1], 1)).T\n",
    "        init_emission_matrix /= col_sums\n",
    "        \n",
    "    if init_state_distribution is None:\n",
    "        n_states = len(set(labeled_states))\n",
    "        init_state_distribution = np.ones(shape=(n_states,))\n",
    "        init_state_distribution /= init_state_distribution.sum()\n",
    "    \n",
    "    \n",
    "    # now we need to do the rest of this\n",
    "    trans_mat = init_transition_matrix\n",
    "    emit_mat = init_emission_matrix\n",
    "    start_state = init_state_distribution\n",
    "    while(True):\n",
    "        forward_probs, scaling = forwards_pass_algorithm(trans_mat, emit_mat, observations, start_state)\n",
    "        backward_prob = backwards_pass_algorithm(trans_mat, emit_mat, observations, scaling)\n",
    "        \n",
    "        #\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\t\txi = np.zeros((nStates,nStates,nSamples-1));\n",
    "\t\tfor t in range(nSamples-1):\n",
    "\t\t\tdenom = np.dot(np.dot(alpha[:,t].T, self.A) * self.B[:,observations[t+1]].T,\n",
    "\t\t\t\t\t\tbeta[:,t+1])\n",
    "\t\t\tfor i in range(nStates):\n",
    "\t\t\t\tnumer = alpha[i,t] * self.A[i,:] * self.B[:,observations[t+1]].T * \\\n",
    "\t\t\t\t\t\tbeta[:,t+1].T\n",
    "\t\t\t\txi[i,:,t] = numer / denom\n",
    "\n",
    "\t\t# gamma_t(i) = P(q_t = S_i | O, hmm)\n",
    "\t\tgamma = np.squeeze(np.sum(xi,axis=1))\n",
    "\t\t# Need final gamma element for new B\n",
    "\t\tprod =  (alpha[:,nSamples-1] * beta[:,nSamples-1]).reshape((-1,1))\n",
    "\t\tgamma = np.hstack((gamma,  prod / np.sum(prod))) #append one more to gamma!!!\n",
    "\n",
    "\t\tnewpi = gamma[:,0]\n",
    "\t\tnewA = np.sum(xi,2) / np.sum(gamma[:,:-1],axis=1).reshape((-1,1))\n",
    "\t\tnewB = copy(B)\n",
    "\n",
    "\t\tnumLevels = self.B.shape[1]\n",
    "\t\tsumgamma = np.sum(gamma,axis=1)\n",
    "\t\tfor lev in range(numLevels):\n",
    "\t\t\tmask = observations == lev\n",
    "\t\t\tnewB[:,lev] = np.sum(gamma[:,mask],axis=1) / sumgamma\n",
    "\n",
    "\t\tif np.max(abs(pi - newpi)) < criterion and \\\n",
    "\t\t\t\tnp.max(abs(A - newA)) < criterion and \\\n",
    "\t\t\t\tnp.max(abs(B - newB)) < criterion:\n",
    "\t\t\tdone = 1;\n",
    "\n",
    "\t\tA[:],B[:],pi[:] = newA,newB,newpi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ENVPy3]",
   "language": "python",
   "name": "conda-env-ENVPy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
